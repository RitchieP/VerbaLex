{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VerbaLex ASR Model","metadata":{}},{"cell_type":"markdown","source":"This notebook is intended to follow the tutorial (Fine tune Whisper model)[https://huggingface.co/blog/fine-tune-whisper] to make sure that I'm able to train a model on Kaggle. The model trained in this notebook is **not a great model** as it is only trained in 10 steps to facilitate debugging and testing.","metadata":{}},{"cell_type":"markdown","source":"# Prepare Environment\n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade datasets accelerate soundfile librosa evaluate jiwer tensorboard gradio transformers","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-20T13:44:34.857265Z","iopub.execute_input":"2024-03-20T13:44:34.857655Z","iopub.status.idle":"2024-03-20T13:45:36.426728Z","shell.execute_reply.started":"2024-03-20T13:44:34.857614Z","shell.execute_reply":"2024-03-20T13:45:36.425582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linking to Huggingface Hub\nLinking this notebook to huggingface hub to enjoy these benefits:\n* Integrated version control: you can be sure that no model checkpoint is lost during training.\n* Tensorboard logs: track important metrics over the course of training.\n* Model cards: document what a model does and its intended use cases.\n* Community: an easy way to share and collaborate with the community!\n\nThis step is also **required** as we need authentication to use a dataset from Huggingface later to fine tune our base model.\n\nTo get the access token for login, head over to your Huggingface profile and press on`Settings`, then `Access Tokens`. Copy your tokens and paste it in the input box when prompted.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HF_TOKEN\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:45:36.428484Z","iopub.execute_input":"2024-03-20T13:45:36.428753Z","iopub.status.idle":"2024-03-20T13:45:37.503351Z","shell.execute_reply.started":"2024-03-20T13:45:36.428730Z","shell.execute_reply":"2024-03-20T13:45:37.502434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset (Base model)\nLoading dataset from Mozilla's Common voice dataset to develop a base ASR model.\n\nDataset link: https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0\n\nThe dataset will need to be streamed from huggingface instead of downloading the whole thing. As it is too large for the Kaggle's notebook storage.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, IterableDatasetDict\n\ndataset = IterableDatasetDict()\n\ndataset[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"train\", use_auth_token=True, streaming=True)\ndataset[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", use_auth_token=True, streaming=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:45:37.504498Z","iopub.execute_input":"2024-03-20T13:45:37.504827Z","iopub.status.idle":"2024-03-20T13:45:51.466009Z","shell.execute_reply.started":"2024-03-20T13:45:37.504803Z","shell.execute_reply":"2024-03-20T13:45:51.465019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize to check that the data is indeed loaded","metadata":{}},{"cell_type":"code","source":"print(list(dataset[\"train\"].take(1)))\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:45:51.468107Z","iopub.execute_input":"2024-03-20T13:45:51.468568Z","iopub.status.idle":"2024-03-20T13:47:08.976062Z","shell.execute_reply.started":"2024-03-20T13:45:51.468542Z","shell.execute_reply":"2024-03-20T13:47:08.975145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning\nAs we can see from the data above, there are a lot of unnecessary data fields. We will drop those columns in the next cell by only selecting the columns that we need.","metadata":{}},{"cell_type":"code","source":"dataset = dataset.select_columns(['audio', 'sentence'])\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:47:08.977343Z","iopub.execute_input":"2024-03-20T13:47:08.978019Z","iopub.status.idle":"2024-03-20T13:47:08.985743Z","shell.execute_reply.started":"2024-03-20T13:47:08.977983Z","shell.execute_reply":"2024-03-20T13:47:08.984694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Feature Extractor, Tokenizer and Data\nThe ASR pipeline can be decomposed into three components.\n1. A feature extractor which pre-process the raw audio inputs. Learn more about it [here](https://huggingface.co/blog/fine-tune-whisper#load-whisperfeatureextractor).\n2. The model which performs the sequence-to-sequence mapping. \n3. A tokenizer which post-process the model outputs to text format. Learn more about it [here](https://huggingface.co/blog/fine-tune-whisper#load-whispertokenizer).\n\nThe Whisper model has already provided a feature extractor, tokenizer as well as a processor. (Which combines the feature extractor and tokenizer, learn more about it [here](https://huggingface.co/blog/fine-tune-whisper#combine-to-create-a-whisperprocessor)) The following section will import those libraries and prepare the pipeline.","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n\n# Load feature extractor to process the raw audio inputs.\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n\n# Load Whisper tokenizer\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:47:08.986944Z","iopub.execute_input":"2024-03-20T13:47:08.987299Z","iopub.status.idle":"2024-03-20T13:47:17.920706Z","shell.execute_reply.started":"2024-03-20T13:47:08.987273Z","shell.execute_reply":"2024-03-20T13:47:17.919586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## This Section is Optional\nOptionally, we can verify that the tokenizer correctly encodes the characters by encoding and decoding a sample from the dataset.\n\n### Decode function\nThere maybe a parameter within the `decode()` function that we are not familiar with, which is `skip_special_tokens`. Special tokens are added during encoding process. The tokenizer appends 'special tokens' at the start and the end of the sentence, including the start/end of the sentence, the language token, as well as any task tokens specified by `WhisperTokenizer.from_pretrained()` function above.\n\nHence, the `skip_special_tokens` parameter will specify whether do we want to include the 'special tokens' when we decode it into normal text.","metadata":{}},{"cell_type":"code","source":"input_str = list(dataset[\"train\"].take(1))[0][\"sentence\"]\n\n# Encoding the sentence\nlabels = tokenizer(input_str).input_ids\n\n# Decoding the sentence.\ndecoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\ndecoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n\nprint(f\"Input:                 {input_str}\")\nprint(f\"Decoded w/ special:    {decoded_with_special}\")\nprint(f\"Decoded w/out special: {decoded_str}\")\nprint(f\"Are equal:             {input_str == decoded_str}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:47:17.922045Z","iopub.execute_input":"2024-03-20T13:47:17.922553Z","iopub.status.idle":"2024-03-20T13:48:01.908872Z","shell.execute_reply.started":"2024-03-20T13:47:17.922527Z","shell.execute_reply":"2024-03-20T13:48:01.907871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data\n","metadata":{}},{"cell_type":"markdown","source":"Since the Whisper model was trained on data with a sampling rate of 16,000, we'll need to resample our data's sampling rate because the data provided by common voice has a sampling rate of 48,000.","metadata":{}},{"cell_type":"code","source":"from datasets import Audio\n\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n\nlist(dataset[\"train\"].take(1))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:48:01.910321Z","iopub.execute_input":"2024-03-20T13:48:01.911251Z","iopub.status.idle":"2024-03-20T13:48:35.727518Z","shell.execute_reply.started":"2024-03-20T13:48:01.911193Z","shell.execute_reply":"2024-03-20T13:48:35.726515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the sampling rate of our data has been resampled to 16,000kHz, we can then proceed to prepare our data for the model.","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    \n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    \n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\ndataset = dataset.map(prepare_dataset)\nlist(dataset[\"train\"].take(1))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:48:35.728828Z","iopub.execute_input":"2024-03-20T13:48:35.729146Z","iopub.status.idle":"2024-03-20T13:49:09.638226Z","shell.execute_reply.started":"2024-03-20T13:48:35.729120Z","shell.execute_reply":"2024-03-20T13:49:09.636837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.remove_columns([\"audio\", \"sentence\"])\nlist(dataset[\"train\"].take(1))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:49:09.641933Z","iopub.execute_input":"2024-03-20T13:49:09.642287Z","iopub.status.idle":"2024-03-20T13:49:43.670710Z","shell.execute_reply.started":"2024-03-20T13:49:09.642259Z","shell.execute_reply":"2024-03-20T13:49:43.669680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Evaluation\n\nFor training, the Huggingface Trainer will handle most of the work for us. All we have to do is:\n\n* Define a data collator. A data collator will take our pre-processed data and prepares PyTorch tensors for the model.\n* Evaluation metrics. We will need to define a `compute_metrics` function that computes the word error rate (WER) metric for evaluation.\n* Load a pre-trained checkpoint and configure it correctly for training.\n* Define training arguments for the Huggingface Trainer to construct a training schedule.","metadata":{}},{"cell_type":"markdown","source":"## Defining a Data Collator","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    \n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        \n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n    \ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:49:43.672001Z","iopub.execute_input":"2024-03-20T13:49:43.672818Z","iopub.status.idle":"2024-03-20T13:49:43.683541Z","shell.execute_reply.started":"2024-03-20T13:49:43.672779Z","shell.execute_reply":"2024-03-20T13:49:43.682679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n    \n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n    \n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n    \n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n    \n    return {\"wer\": wer}","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:49:43.685923Z","iopub.execute_input":"2024-03-20T13:49:43.686222Z","iopub.status.idle":"2024-03-20T13:49:47.396922Z","shell.execute_reply.started":"2024-03-20T13:49:43.686193Z","shell.execute_reply":"2024-03-20T13:49:47.396124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load a pre-trained checkpoint","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n\nmodel.generation_config.language = \"<|en|>\"\nmodel.generation_config.task = \"transcribe\"","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:49:47.398626Z","iopub.execute_input":"2024-03-20T13:49:47.399001Z","iopub.status.idle":"2024-03-20T13:50:12.655210Z","shell.execute_reply.started":"2024-03-20T13:49:47.398969Z","shell.execute_reply":"2024-03-20T13:50:12.654358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define training arguments","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-small-eng-gen\",  # change to a repo name of your choice\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n    learning_rate=1e-5,\n    warmup_steps=1,\n    max_steps=10,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=5,\n    eval_steps=5,\n    logging_steps=5,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:50:12.656743Z","iopub.execute_input":"2024-03-20T13:50:12.657332Z","iopub.status.idle":"2024-03-20T13:50:12.718435Z","shell.execute_reply.started":"2024-03-20T13:50:12.657303Z","shell.execute_reply":"2024-03-20T13:50:12.717530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:50:12.719904Z","iopub.execute_input":"2024-03-20T13:50:12.720531Z","iopub.status.idle":"2024-03-20T13:50:13.362646Z","shell.execute_reply.started":"2024-03-20T13:50:12.720502Z","shell.execute_reply":"2024-03-20T13:50:13.361626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n","metadata":{}},{"cell_type":"code","source":"trainer.train() ","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:50:13.363872Z","iopub.execute_input":"2024-03-20T13:50:13.364210Z","iopub.status.idle":"2024-03-20T13:51:40.121356Z","shell.execute_reply.started":"2024-03-20T13:50:13.364177Z","shell.execute_reply":"2024-03-20T13:51:40.119624Z"},"trusted":true},"execution_count":null,"outputs":[]}]}