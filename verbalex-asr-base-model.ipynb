{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7724902,"sourceType":"datasetVersion","datasetId":4512909}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VerbaLex ASR Model","metadata":{}},{"cell_type":"markdown","source":"# Prepare Environment\n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade datasets transformers accelerate soundfile librosa evaluate jiwer tensorboard gradio","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linking to Huggingface Hub\nLinking this notebook to huggingface hub to enjoy these benefits:\n* Integrated version control: you can be sure that no model checkpoint is lost during training.\n* Tensorboard logs: track important metrics over the course of training.\n* Model cards: document what a model does and its intended use cases.\n* Community: an easy way to share and collaborate with the community!\n\nThis step is also **required** as we need authentication to use a dataset from Huggingface later to fine tune our base model.\n\nTo get the access token for login, head over to your Huggingface profile and press on`Settings`, then `Access Tokens`. Copy your tokens and paste it in the input box when prompted.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset (Base model)\nLoading dataset from Mozilla's Common voice dataset to develop a base ASR model.\n\nDataset link: https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0\n\nThe dataset will need to be streamed from huggingface instead of downloading the whole thing. As it is too large for the Kaggle's notebook storage.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ncommon_voice_train = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"train\", use_auth_token=True, streaming=True)\ncommon_voice_test = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", use_auth_token=True, streaming=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:23:33.005996Z","iopub.execute_input":"2024-03-14T07:23:33.006365Z","iopub.status.idle":"2024-03-14T07:23:37.233906Z","shell.execute_reply.started":"2024-03-14T07:23:33.006337Z","shell.execute_reply":"2024-03-14T07:23:37.233118Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"dataset_head = common_voice_train.take(1)\nlist(dataset_head)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:50:20.872476Z","iopub.execute_input":"2024-03-14T07:50:20.872873Z","iopub.status.idle":"2024-03-14T07:50:20.920879Z","shell.execute_reply.started":"2024-03-14T07:50:20.872834Z","shell.execute_reply":"2024-03-14T07:50:20.919725Z"},"trusted":true},"execution_count":39,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_head \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mlist\u001b[39m(dataset_head)\n","\u001b[0;31mKeyError\u001b[0m: 0"],"ename":"KeyError","evalue":"0","output_type":"error"}]},{"cell_type":"markdown","source":"# Data Cleaning\nAs we can see from the data above, there are a lot of unnecessary data fields. We will drop those columns in the next cell by only selecting the columns that we need.","metadata":{}},{"cell_type":"code","source":"common_voice_train = common_voice_train.select_columns(['audio', 'sentence'])\ncommon_voice_test = common_voice_test.select_columns(['audio', 'sentence'])\nprint(list(common_voice_train.take(1)))\nprint(list(common_voice_test.take(1)))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:11:16.015069Z","iopub.execute_input":"2024-03-14T07:11:16.015378Z","iopub.status.idle":"2024-03-14T07:11:33.099437Z","shell.execute_reply.started":"2024-03-14T07:11:16.015352Z","shell.execute_reply":"2024-03-14T07:11:33.098470Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Reading metadata...: 1013968it [00:15, 65115.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"[{'audio': {'path': 'en_train_0/common_voice_en_21635524.mp3', 'array': array([ 0.00000000e+00, -1.64753278e-14, -5.40568024e-14, ...,\n        7.96796940e-08, -1.15487887e-06, -8.86387454e-07]), 'sampling_rate': 48000}, 'sentence': 'This device has a cathode inside an anode wire cage.'}]\n","output_type":"stream"},{"name":"stderr","text":"Reading metadata...: 16372it [00:00, 28890.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"[{'audio': {'path': 'en_test_0/common_voice_en_27710027.mp3', 'array': array([-7.95807864e-13, -1.59161573e-12, -6.25277607e-12, ...,\n        1.52416442e-06,  1.94649760e-06,  1.25737506e-06]), 'sampling_rate': 48000}, 'sentence': 'Joe Keaton disapproved of films, and Buster also had reservations about the medium.'}]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare Feature Extractor, Tokenizer and Data\nThe ASR pipeline can be decomposed into three components.\n1. A feature extractor which pre-process the raw audio inputs. Learn more about it [here](https://huggingface.co/blog/fine-tune-whisper#load-whisperfeatureextractor).\n2. The model which performs the sequence-to-sequence mapping. \n3. A tokenizer which post-process the model outputs to text format. Learn more about it [here](https://huggingface.co/blog/fine-tune-whisper#load-whispertokenizer).\n\nThe Whisper model has already provided a feature extractor, tokenizer as well as a processor. (Which combines the feature extractor and tokenizer, learn more about it [here](https://huggingface.co/blog/fine-tune-whisper#combine-to-create-a-whisperprocessor)) The following section will import those libraries and prepare the pipeline.","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n\n# Load feature extractor to process the raw audio inputs.\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n\n# Load Whisper tokenizer\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:11:33.102202Z","iopub.execute_input":"2024-03-14T07:11:33.102496Z","iopub.status.idle":"2024-03-14T07:11:34.712172Z","shell.execute_reply.started":"2024-03-14T07:11:33.102471Z","shell.execute_reply":"2024-03-14T07:11:34.711332Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## This Section is Optional\nOptionally, we can verify that the tokenizer correctly encodes the characters by encoding and decoding a sample from the dataset.\n\n### Decode function\nThere maybe a parameter within the `decode()` function that we are not familiar with, which is `skip_special_tokens`. Special tokens are added during encoding process. The tokenizer appends 'special tokens' at the start and the end of the sentence, including the start/end of the sentence, the language token, as well as any task tokens specified by `WhisperTokenizer.from_pretrained()` function above.\n\nHence, the `skip_special_tokens` parameter will specify whether do we want to include the 'special tokens' when we decode it into normal text.","metadata":{}},{"cell_type":"code","source":"input_str = list(common_voice_train.take(1))[0][\"sentence\"]\n\n# Encoding the sentence\nlabels = tokenizer(input_str).input_ids\n\n# Decoding the sentence.\ndecoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\ndecoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n\nprint(f\"Input:                 {input_str}\")\nprint(f\"Decoded w/ special:    {decoded_with_special}\")\nprint(f\"Decoded w/out special: {decoded_str}\")\nprint(f\"Are equal:             {input_str == decoded_str}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:11:34.713277Z","iopub.execute_input":"2024-03-14T07:11:34.713577Z","iopub.status.idle":"2024-03-14T07:11:52.598602Z","shell.execute_reply.started":"2024-03-14T07:11:34.713553Z","shell.execute_reply":"2024-03-14T07:11:52.597641Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Reading metadata...: 1013968it [00:17, 58832.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Input:                 This device has a cathode inside an anode wire cage.\nDecoded w/ special:    <|startoftranscript|><|en|><|transcribe|><|notimestamps|>This device has a cathode inside an anode wire cage.<|endoftext|>\nDecoded w/out special: This device has a cathode inside an anode wire cage.\nAre equal:             True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare data\n","metadata":{}},{"cell_type":"markdown","source":"Since the Whisper model was trained on data with a sampling rate of 16,000, we'll need to resample our data's sampling rate because the data provided by common voice has a sampling rate of 48,000.","metadata":{}},{"cell_type":"code","source":"from datasets import Audio\n\ncommon_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16000))\ncommon_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16000))\n\nlist(common_voice_train.take(1))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:11:52.599957Z","iopub.execute_input":"2024-03-14T07:11:52.601607Z","iopub.status.idle":"2024-03-14T07:12:09.783932Z","shell.execute_reply.started":"2024-03-14T07:11:52.601578Z","shell.execute_reply":"2024-03-14T07:12:09.782540Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Reading metadata...: 1013968it [00:16, 61527.18it/s]\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[{'audio': {'path': 'en_train_0/common_voice_en_21635524.mp3',\n   'array': array([-1.13686838e-13,  1.47792889e-12,  0.00000000e+00, ...,\n           1.59220781e-06,  3.68470864e-07, -8.33906597e-08]),\n   'sampling_rate': 16000},\n  'sentence': 'This device has a cathode inside an anode wire cage.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"After the sampling rate of our data has been resampled to 16,000kHz, we can then proceed to prepare our data for the model.","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    \n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    \n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\ncommon_voice_train = common_voice_train.map(prepare_dataset)\ncommon_voice_test = common_voice_test.map(prepare_dataset)\nlist(common_voice_train.take(1))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:12:09.785304Z","iopub.execute_input":"2024-03-14T07:12:09.785650Z","iopub.status.idle":"2024-03-14T07:12:09.845531Z","shell.execute_reply.started":"2024-03-14T07:12:09.785620Z","shell.execute_reply":"2024-03-14T07:12:09.843512Z"},"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[0;32m----> 9\u001b[0m common_voice_train \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_voice_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m common_voice_test \u001b[38;5;241m=\u001b[39m common_voice_test\u001b[38;5;241m.\u001b[39mmap(prepare_dataset, num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mlist\u001b[39m(common_voice_train\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m))\n","\u001b[0;31mTypeError\u001b[0m: IterableDataset.map() got an unexpected keyword argument 'num_proc'"],"ename":"TypeError","evalue":"IterableDataset.map() got an unexpected keyword argument 'num_proc'","output_type":"error"}]},{"cell_type":"markdown","source":"# Training and Evaluation\n\nFor training, the Huggingface Trainer will handle most of the work for us. All we have to do is:\n\n* Define a data collator. A data collator will take our pre-processed data and prepares PyTorch tensors for the model.\n* Evaluation metrics. We will need to define a `compute_metrics` function that computes the word error rate (WER) metric for evaluation.\n* Load a pre-trained checkpoint and configure it correctly for training.\n* Define training arguments for the Huggingface Trainer to construct a training schedule.","metadata":{}},{"cell_type":"markdown","source":"## Defining a Data Collator","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    \n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        \n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n    \ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:12:09.846654Z","iopub.status.idle":"2024-03-14T07:12:09.847439Z","shell.execute_reply.started":"2024-03-14T07:12:09.847150Z","shell.execute_reply":"2024-03-14T07:12:09.847177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n    \n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n    \n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n    \n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n    \n    return {\"wer\": wer}","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:12:09.848906Z","iopub.status.idle":"2024-03-14T07:12:09.849314Z","shell.execute_reply.started":"2024-03-14T07:12:09.849130Z","shell.execute_reply":"2024-03-14T07:12:09.849149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load a pre-trained checkpoint","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n\nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\nmodel.generation_config.language = \"en\"","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:12:09.850504Z","iopub.status.idle":"2024-03-14T07:12:09.850895Z","shell.execute_reply.started":"2024-03-14T07:12:09.850686Z","shell.execute_reply":"2024-03-14T07:12:09.850703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define training arguments","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-small-eng-gen\",  # change to a repo name of your choice\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n    learning_rate=1e-5,\n    warmup_steps=500,\n    max_steps=2000,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=1000,\n    eval_steps=1000,\n    logging_steps=25,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    push_to_hub=True,\n    ignore_data_skip=True,\n    do_eval=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:12:09.852707Z","iopub.status.idle":"2024-03-14T07:12:09.853239Z","shell.execute_reply.started":"2024-03-14T07:12:09.852968Z","shell.execute_reply":"2024-03-14T07:12:09.852993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=common_voice_train,\n    eval_dataset=common_voice_test,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:12:09.855415Z","iopub.status.idle":"2024-03-14T07:12:09.855951Z","shell.execute_reply.started":"2024-03-14T07:12:09.855655Z","shell.execute_reply":"2024-03-14T07:12:09.855679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n","metadata":{}},{"cell_type":"code","source":"trainer.train() ","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:12:09.857112Z","iopub.status.idle":"2024-03-14T07:12:09.857614Z","shell.execute_reply.started":"2024-03-14T07:12:09.857350Z","shell.execute_reply":"2024-03-14T07:12:09.857374Z"},"trusted":true},"execution_count":null,"outputs":[]}]}